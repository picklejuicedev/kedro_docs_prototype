{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Spaceflights Docs The Spaceflights Docs project is a prototype static documentation for the space flight tutorial . I chose this project as many people will be familiar with it, or if not, can easily review it through the online documentation or run it using the spaceflight starter . This static site is mostly manually edited to explore what functionality a documentation generator would need to cover. The Prerequisites and Usage are only needed if you wish to locally generate the documentation and play with the mkdocs framework. Within the data section you can explore the datasets used by this project. Extra points for those critical readers that can spot why model_input_table has grown to over 750k rows. Parameters make use of modular pipelines and the documentation can explain the detailed use of them. To explore the pipelines a static version of Kedro-Viz has been included. And finally there are some more thoughts on why I think documentation is important and how this could be useful. Note Some of these pages are mock documentation to give it some meat . Apologies if some points are repeated. Index Goals of the project Prerequisites Input, Intermediate and Output Data Processing pipeline Usage and Parameters About Readme","title":"Home"},{"location":"#welcome-to-spaceflights-docs","text":"The Spaceflights Docs project is a prototype static documentation for the space flight tutorial . I chose this project as many people will be familiar with it, or if not, can easily review it through the online documentation or run it using the spaceflight starter . This static site is mostly manually edited to explore what functionality a documentation generator would need to cover. The Prerequisites and Usage are only needed if you wish to locally generate the documentation and play with the mkdocs framework. Within the data section you can explore the datasets used by this project. Extra points for those critical readers that can spot why model_input_table has grown to over 750k rows. Parameters make use of modular pipelines and the documentation can explain the detailed use of them. To explore the pipelines a static version of Kedro-Viz has been included. And finally there are some more thoughts on why I think documentation is important and how this could be useful. Note Some of these pages are mock documentation to give it some meat . Apologies if some points are repeated.","title":"Welcome to Spaceflights Docs"},{"location":"#index","text":"Goals of the project Prerequisites Input, Intermediate and Output Data Processing pipeline Usage and Parameters About Readme","title":"Index"},{"location":"README_copy/","text":"Note This is an example how external files can be incorporated into the documentation. README.md is outside the docs\\ folder, but can be included with a special plugin. spaceflights docs prototype Overview Below is the template Readme generated by the spaceflights starter. Find the GitHub pages containing the full documentation here . This is your new Kedro project, which was generated using Kedro 0.18.3 . Take a look at the Kedro documentation to get started. Rules and guidelines In order to get the best out of the template: Don't remove any lines from the .gitignore file we provide Make sure your results can be reproduced by following a data engineering convention Don't commit data to your repository Don't commit any credentials or your local configuration to your repository. Keep all your credentials and local configuration in conf/local/ How to install dependencies Declare any dependencies in src/requirements.txt for pip installation and src/environment.yml for conda installation. To install them, run: pip install -r src/requirements.txt How to run your Kedro pipeline You can run your Kedro project with: kedro run How to test your Kedro project Have a look at the file src/tests/test_run.py for instructions on how to write your tests. You can run your tests as follows: kedro test To configure the coverage threshold, go to the .coveragerc file. Project dependencies To generate or update the dependency requirements for your project: kedro build-reqs This will pip-compile the contents of src/requirements.txt into a new file src/requirements.lock . You can see the output of the resolution by opening src/requirements.lock . After this, if you'd like to update your project requirements, please update src/requirements.txt and re-run kedro build-reqs . Further information about project dependencies How to work with Kedro and notebooks Note: Using kedro jupyter or kedro ipython to run your notebook provides these variables in scope: catalog , context , pipelines and session . Jupyter, JupyterLab, and IPython are already included in the project requirements by default, so once you have run pip install -r src/requirements.txt you will not need to take any extra steps before you use them. Jupyter To use Jupyter notebooks in your Kedro project, you need to install Jupyter: pip install jupyter After installing Jupyter, you can start a local notebook server: kedro jupyter notebook JupyterLab To use JupyterLab, you need to install it: pip install jupyterlab You can also start JupyterLab: kedro jupyter lab IPython And if you want to run an IPython session: kedro ipython How to convert notebook cells to nodes in a Kedro project You can move notebook code over into a Kedro project structure using a mixture of cell tagging and Kedro CLI commands. By adding the node tag to a cell and running the command below, the cell's source code will be copied over to a Python file within src/<package_name>/nodes/ : kedro jupyter convert <filepath_to_my_notebook> Note: The name of the Python file matches the name of the original notebook. Alternatively, you may want to transform all your notebooks in one go. Run the following command to convert all notebook files found in the project root directory and under any of its sub-folders: kedro jupyter convert --all How to ignore notebook output cells in git To automatically strip out all output cell contents before committing to git , you can run kedro activate-nbstripout . This will add a hook in .git/config which will run nbstripout before anything is committed to git . Note: Your output cells will be retained locally. Package your Kedro project Further information about building project documentation and packaging your project","title":"Readme"},{"location":"README_copy/#spaceflights-docs-prototype","text":"","title":"spaceflights docs prototype"},{"location":"README_copy/#overview","text":"Below is the template Readme generated by the spaceflights starter. Find the GitHub pages containing the full documentation here . This is your new Kedro project, which was generated using Kedro 0.18.3 . Take a look at the Kedro documentation to get started.","title":"Overview"},{"location":"README_copy/#rules-and-guidelines","text":"In order to get the best out of the template: Don't remove any lines from the .gitignore file we provide Make sure your results can be reproduced by following a data engineering convention Don't commit data to your repository Don't commit any credentials or your local configuration to your repository. Keep all your credentials and local configuration in conf/local/","title":"Rules and guidelines"},{"location":"README_copy/#how-to-install-dependencies","text":"Declare any dependencies in src/requirements.txt for pip installation and src/environment.yml for conda installation. To install them, run: pip install -r src/requirements.txt","title":"How to install dependencies"},{"location":"README_copy/#how-to-run-your-kedro-pipeline","text":"You can run your Kedro project with: kedro run","title":"How to run your Kedro pipeline"},{"location":"README_copy/#how-to-test-your-kedro-project","text":"Have a look at the file src/tests/test_run.py for instructions on how to write your tests. You can run your tests as follows: kedro test To configure the coverage threshold, go to the .coveragerc file.","title":"How to test your Kedro project"},{"location":"README_copy/#project-dependencies","text":"To generate or update the dependency requirements for your project: kedro build-reqs This will pip-compile the contents of src/requirements.txt into a new file src/requirements.lock . You can see the output of the resolution by opening src/requirements.lock . After this, if you'd like to update your project requirements, please update src/requirements.txt and re-run kedro build-reqs . Further information about project dependencies","title":"Project dependencies"},{"location":"README_copy/#how-to-work-with-kedro-and-notebooks","text":"Note: Using kedro jupyter or kedro ipython to run your notebook provides these variables in scope: catalog , context , pipelines and session . Jupyter, JupyterLab, and IPython are already included in the project requirements by default, so once you have run pip install -r src/requirements.txt you will not need to take any extra steps before you use them.","title":"How to work with Kedro and notebooks"},{"location":"README_copy/#jupyter","text":"To use Jupyter notebooks in your Kedro project, you need to install Jupyter: pip install jupyter After installing Jupyter, you can start a local notebook server: kedro jupyter notebook","title":"Jupyter"},{"location":"README_copy/#jupyterlab","text":"To use JupyterLab, you need to install it: pip install jupyterlab You can also start JupyterLab: kedro jupyter lab","title":"JupyterLab"},{"location":"README_copy/#ipython","text":"And if you want to run an IPython session: kedro ipython","title":"IPython"},{"location":"README_copy/#how-to-convert-notebook-cells-to-nodes-in-a-kedro-project","text":"You can move notebook code over into a Kedro project structure using a mixture of cell tagging and Kedro CLI commands. By adding the node tag to a cell and running the command below, the cell's source code will be copied over to a Python file within src/<package_name>/nodes/ : kedro jupyter convert <filepath_to_my_notebook> Note: The name of the Python file matches the name of the original notebook. Alternatively, you may want to transform all your notebooks in one go. Run the following command to convert all notebook files found in the project root directory and under any of its sub-folders: kedro jupyter convert --all","title":"How to convert notebook cells to nodes in a Kedro project"},{"location":"README_copy/#how-to-ignore-notebook-output-cells-in-git","text":"To automatically strip out all output cell contents before committing to git , you can run kedro activate-nbstripout . This will add a hook in .git/config which will run nbstripout before anything is committed to git . Note: Your output cells will be retained locally.","title":"How to ignore notebook output cells in git"},{"location":"README_copy/#package-your-kedro-project","text":"Further information about building project documentation and packaging your project","title":"Package your Kedro project"},{"location":"about/","text":"About Spaceflights Creators: The Team at Kedro + Community License: Apache 2.0","title":"About"},{"location":"about/#about-spaceflights","text":"Creators: The Team at Kedro + Community License: Apache 2.0","title":"About Spaceflights"},{"location":"doccreation/","text":"A new doc system Creating documentation is not everyones favourite - it takes time away from valuable dev time and is so quickly out of date. Will anybody read it? Or are we just documenting to tick a box? Why documentation is a first class citizen I find it very helpful for any serious software or data project to adapt a product mindset. You embark on a journey and develop this thing that solves a problem. During development assumptions are made, solutions desigend and tested until finally it solves the problem that it was tasked to solve. Unless it was a one-shot project and never to be used again, there will be the need to communicate to others how it works and what it does, what assumptions were made during the design and how it can be used. Without this, the project will slowly wither away and die, especially when the original designers aren't around any more. So the question is: Where and how do we write stuff down? Anatomy of a Kedro project Let's consider the different parts of a Kero project: Pipelines and Nodes - the meaty stuff! Settings - all those yaml files Data - lots of it, in many different forms Setup, Installation, Deployment - How to install or deploy it Testing - how to ensure it works correctly Usage - how should one use it, and what for The great news is that for many of those sections Kedro has an answer for how it should be done, committed to code and stored in a repository. So if you are a developer and understand the framework you can look up most things and make sense of the code together maybe with a high level Readme.md in the root of the repository. This may be ok for small projects but for anything more significant we need some kind of docs framework. Also what to do when some of your customers aren't developers that are familiar with repositories? How can you include them into the fold? Write as little as possible So it is paramount that a documentation system can extract all the inherent information in a Kedro project and publish this automatically in such a way that it is readable by newcomers to the project and external stakeholders. So for example, the catalog defines all DataSet resources - they can easily be referenced and auto-generate documentation that then can be annotated with context. nodes and pipelines have docstrings and those can be used to explain how the parts of the processing work. KedroViz has a wealth of information that is best explored interactively. Make it easy to write the rest Many things just need to be written, but there is nothing more daunting than a clean white page. If we have a well documented template (or multiple templates for different types of projects) then architectural decisions have a home, assumptions can be written up quickly. Just as with pushing a developer towards using standard building blocks when creating data processing pipelines (nodes, pipelines, DataSets, etc) they should also be encourage to adopt a certain documentation structure. Consistency will then make both the writing easier and quicker, but also the reader will know where to find the information they seek as it is in the same place. And design better products Have you ever tried to document a half-cooked idea? Doesn't work! So documentation (just like testing) is a great tool to review your own work and test it for public consumption. It is one thing to make something work, quite another to build and explain it in such a way that others can use it easily and without errors. The job is only complete when you are not needed any more and the project can life on its own. Building blocks mkdocs documentation system based on .md documents in /docs folder write anything and it will compile into a nice looking webpage publish to github pages easily be able to include root/Readme.md as index (if possible) use mkdocsstrings to parse code and add to documentation parse settings (conf) and annotate with comments parse catalog and provide framework for describing datasets provide boilerplate documentation for building, deploying and installing provide notebook index with comments use autogeneration to create base documentation but be able to manually annotate","title":"More thoughts"},{"location":"doccreation/#a-new-doc-system","text":"Creating documentation is not everyones favourite - it takes time away from valuable dev time and is so quickly out of date. Will anybody read it? Or are we just documenting to tick a box?","title":"A new doc system"},{"location":"doccreation/#why-documentation-is-a-first-class-citizen","text":"I find it very helpful for any serious software or data project to adapt a product mindset. You embark on a journey and develop this thing that solves a problem. During development assumptions are made, solutions desigend and tested until finally it solves the problem that it was tasked to solve. Unless it was a one-shot project and never to be used again, there will be the need to communicate to others how it works and what it does, what assumptions were made during the design and how it can be used. Without this, the project will slowly wither away and die, especially when the original designers aren't around any more. So the question is: Where and how do we write stuff down?","title":"Why documentation is a first class citizen"},{"location":"doccreation/#anatomy-of-a-kedro-project","text":"Let's consider the different parts of a Kero project: Pipelines and Nodes - the meaty stuff! Settings - all those yaml files Data - lots of it, in many different forms Setup, Installation, Deployment - How to install or deploy it Testing - how to ensure it works correctly Usage - how should one use it, and what for The great news is that for many of those sections Kedro has an answer for how it should be done, committed to code and stored in a repository. So if you are a developer and understand the framework you can look up most things and make sense of the code together maybe with a high level Readme.md in the root of the repository. This may be ok for small projects but for anything more significant we need some kind of docs framework. Also what to do when some of your customers aren't developers that are familiar with repositories? How can you include them into the fold?","title":"Anatomy of a Kedro project"},{"location":"doccreation/#write-as-little-as-possible","text":"So it is paramount that a documentation system can extract all the inherent information in a Kedro project and publish this automatically in such a way that it is readable by newcomers to the project and external stakeholders. So for example, the catalog defines all DataSet resources - they can easily be referenced and auto-generate documentation that then can be annotated with context. nodes and pipelines have docstrings and those can be used to explain how the parts of the processing work. KedroViz has a wealth of information that is best explored interactively.","title":"Write as little as possible"},{"location":"doccreation/#make-it-easy-to-write-the-rest","text":"Many things just need to be written, but there is nothing more daunting than a clean white page. If we have a well documented template (or multiple templates for different types of projects) then architectural decisions have a home, assumptions can be written up quickly. Just as with pushing a developer towards using standard building blocks when creating data processing pipelines (nodes, pipelines, DataSets, etc) they should also be encourage to adopt a certain documentation structure. Consistency will then make both the writing easier and quicker, but also the reader will know where to find the information they seek as it is in the same place.","title":"Make it easy to write the rest"},{"location":"doccreation/#and-design-better-products","text":"Have you ever tried to document a half-cooked idea? Doesn't work! So documentation (just like testing) is a great tool to review your own work and test it for public consumption. It is one thing to make something work, quite another to build and explain it in such a way that others can use it easily and without errors. The job is only complete when you are not needed any more and the project can life on its own.","title":"And design better products"},{"location":"doccreation/#building-blocks","text":"mkdocs documentation system based on .md documents in /docs folder write anything and it will compile into a nice looking webpage publish to github pages easily be able to include root/Readme.md as index (if possible) use mkdocsstrings to parse code and add to documentation parse settings (conf) and annotate with comments parse catalog and provide framework for describing datasets provide boilerplate documentation for building, deploying and installing provide notebook index with comments use autogeneration to create base documentation but be able to manually annotate","title":"Building blocks"},{"location":"goals/","text":"Goals of Spaceflights The main goal for spaceflights is to get you up and running quickly with the Kedro framework. You will have explored writing nodes and pipelines , hopefullly created your first tests and played with modular pipelines and namespaces. This project is based on the spaceflight starter , which was modified to hold this prototype documentation. The idea is to have a well known project as example for how a to mix automatically generated documentation with manually entered pages or sections. So whilst currently it is mostly manually curated, the idea is to automate as many steps as possible, so the docs-generator can create placeholders for manual text as well as insert links to the automatically generated documents. The data pages were generated using some scripts, so you can have a peek at the notebooks. Please leave comments on slack or create issues on the github page .","title":"Goals"},{"location":"goals/#goals-of-spaceflights","text":"The main goal for spaceflights is to get you up and running quickly with the Kedro framework. You will have explored writing nodes and pipelines , hopefullly created your first tests and played with modular pipelines and namespaces. This project is based on the spaceflight starter , which was modified to hold this prototype documentation. The idea is to have a well known project as example for how a to mix automatically generated documentation with manually entered pages or sections. So whilst currently it is mostly manually curated, the idea is to automate as many steps as possible, so the docs-generator can create placeholders for manual text as well as insert links to the automatically generated documents. The data pages were generated using some scripts, so you can have a peek at the notebooks. Please leave comments on slack or create issues on the github page .","title":"Goals of Spaceflights"},{"location":"prerequisites/","text":"Getting set up Note You only need to do this if you want to play with documentation system, update or change the content and preview locally. This project can be found at https://github.com/picklejuicedev/kedro_docs_prototype.git Either clone or download it to a local folder. If you want to play with the documentation locally, install the following packages: - `mkdocs` # core document library - `mkdocstrings` # used to extract `docstrings` from code - `mkdocstring-python` # python extension for `docstrings` - `mkdocs-gen-files` # helper to create markdown files programatically - `mkdocs-material` # very nice material theme - `mkdocs-include-markdown-plugin` # plugin to insert .md or other files into markdown files Like this: pip install mkdocs mkdocstrings mkdocstring-python mkdocs-gen-files mkdocs-material mkdocs-include-markdown-plugin Kedro requirements You don't need to run the kedro project to be able to generate the documentation. But if you want to run kedro then you also need to do the usual pip install -m src/requirements.txt .","title":"Prerequisites"},{"location":"prerequisites/#getting-set-up","text":"Note You only need to do this if you want to play with documentation system, update or change the content and preview locally. This project can be found at https://github.com/picklejuicedev/kedro_docs_prototype.git Either clone or download it to a local folder. If you want to play with the documentation locally, install the following packages: - `mkdocs` # core document library - `mkdocstrings` # used to extract `docstrings` from code - `mkdocstring-python` # python extension for `docstrings` - `mkdocs-gen-files` # helper to create markdown files programatically - `mkdocs-material` # very nice material theme - `mkdocs-include-markdown-plugin` # plugin to insert .md or other files into markdown files Like this: pip install mkdocs mkdocstrings mkdocstring-python mkdocs-gen-files mkdocs-material mkdocs-include-markdown-plugin","title":"Getting set up"},{"location":"prerequisites/#kedro-requirements","text":"You don't need to run the kedro project to be able to generate the documentation. But if you want to run kedro then you also need to do the usual pip install -m src/requirements.txt .","title":"Kedro requirements"},{"location":"usage/","text":"Usage Note Normally you wold discuss how to use the Kedro project here, i.e. the spaceflights code and/or model. However let's use it to highlight how to use the documentation system. Run a local server Assuming you have installed all prerequisites , you can now run a local test server: mkdocs serve It will collect the documentation and serve them up to a local host: (docs-prototype-py3.10) PS C:\\code\\kedro\\docs_prototype\\spaceflights> mkdocs serve INFO - Building documentation... INFO - Cleaning site directory INFO - Documentation built in 0.95 seconds INFO - [16:15:17] Watching paths for changes: 'docs', 'mkdocs.yml', 'README.md' INFO - [16:15:17] Serving on http://127.0.0.1:8000/ Open a browser and point to http://127.0.0.1:8000/ and you should see the documentation website. This page will auto-reload on any changes, so try modifying any of the .md files in the /docs folder and on save it will be reflected on the site. This is a fast and easy workflow to get all manual docs written and check the automatically generated ones look correct. Deploy docs manually Once happy with the documentation you can build the site manually using: mkdocs build It will create a static site in the subfolder /site . Use your ftp client or other tool to upload to your hosting platform. Deploy automatically using github workflows For this project a github workflow has been added (see /.github/workflows/ci.yml ). It will automatically build the documentation and deploy to the gh-pages branch so will be visible at <username>.github.io/<repository> . Changing the theme mkdocs comes with the excellent readthedocs theme and this is running here by default. You can switch to the material theme by changing the theme entry in mkdocs.yml to material : ... theme: material ... More help Have a look at the excellent mkdocs website for more help. Hopefully this shold get you started.","title":"Usage"},{"location":"usage/#usage","text":"Note Normally you wold discuss how to use the Kedro project here, i.e. the spaceflights code and/or model. However let's use it to highlight how to use the documentation system.","title":"Usage"},{"location":"usage/#run-a-local-server","text":"Assuming you have installed all prerequisites , you can now run a local test server: mkdocs serve It will collect the documentation and serve them up to a local host: (docs-prototype-py3.10) PS C:\\code\\kedro\\docs_prototype\\spaceflights> mkdocs serve INFO - Building documentation... INFO - Cleaning site directory INFO - Documentation built in 0.95 seconds INFO - [16:15:17] Watching paths for changes: 'docs', 'mkdocs.yml', 'README.md' INFO - [16:15:17] Serving on http://127.0.0.1:8000/ Open a browser and point to http://127.0.0.1:8000/ and you should see the documentation website. This page will auto-reload on any changes, so try modifying any of the .md files in the /docs folder and on save it will be reflected on the site. This is a fast and easy workflow to get all manual docs written and check the automatically generated ones look correct.","title":"Run a local server"},{"location":"usage/#deploy-docs-manually","text":"Once happy with the documentation you can build the site manually using: mkdocs build It will create a static site in the subfolder /site . Use your ftp client or other tool to upload to your hosting platform.","title":"Deploy docs manually"},{"location":"usage/#deploy-automatically-using-github-workflows","text":"For this project a github workflow has been added (see /.github/workflows/ci.yml ). It will automatically build the documentation and deploy to the gh-pages branch so will be visible at <username>.github.io/<repository> .","title":"Deploy automatically using github workflows"},{"location":"usage/#changing-the-theme","text":"mkdocs comes with the excellent readthedocs theme and this is running here by default. You can switch to the material theme by changing the theme entry in mkdocs.yml to material : ... theme: material ...","title":"Changing the theme"},{"location":"usage/#more-help","text":"Have a look at the excellent mkdocs website for more help. Hopefully this shold get you started.","title":"More help"},{"location":"data/_Data_overview/","text":"Note Not sure how to handle all the different types of datasets. What may work is to have a generic front page like this one simply listing the catalog and data types, but then have a separate pages for more details. We can autogenerate some from the data itself i.e. using pandas-profiling or some simple pandas commands. This should be extendable for other datasets to enable automatic generation. raw Name Type Path Details companies pandas.CSVDataSet data/01_raw/companies.csv basic info , pandas profiling reviews pandas.CSVDataSet data/01_raw/reviews.csv basic info , pandas profiling shuttles pandas.ExcelDataSet data/01_raw/shuttles.xlsx basic info , pandas profiling intermediate Name Type Path Details preprocessed_companies pandas.ParquetDataSe data/02_intermediate/preprocessed_companies.pq preprocessed_shuttles pandas.ParquetDataSet data/02_intermediate/preprocessed_shuttles.pq primary Name Type Path Details model_input_table pandas.ParquetDataSet data/03_primary/model_input_table.pq basic info , pandas profiling models Name Type Path Details active_modelling_pipeline.regressor pickle.PickleDataSet data/06_models/regressor_active.pickle regressor_candidate.regressor pickle.PickleDataSet data/06_models/regressor_candidate.pickle","title":" Data overview"},{"location":"data/_Data_overview/#raw","text":"Name Type Path Details companies pandas.CSVDataSet data/01_raw/companies.csv basic info , pandas profiling reviews pandas.CSVDataSet data/01_raw/reviews.csv basic info , pandas profiling shuttles pandas.ExcelDataSet data/01_raw/shuttles.xlsx basic info , pandas profiling","title":"raw"},{"location":"data/_Data_overview/#intermediate","text":"Name Type Path Details preprocessed_companies pandas.ParquetDataSe data/02_intermediate/preprocessed_companies.pq preprocessed_shuttles pandas.ParquetDataSet data/02_intermediate/preprocessed_shuttles.pq","title":"intermediate"},{"location":"data/_Data_overview/#primary","text":"Name Type Path Details model_input_table pandas.ParquetDataSet data/03_primary/model_input_table.pq basic info , pandas profiling","title":"primary"},{"location":"data/_Data_overview/#models","text":"Name Type Path Details active_modelling_pipeline.regressor pickle.PickleDataSet data/06_models/regressor_active.pickle regressor_candidate.regressor pickle.PickleDataSet data/06_models/regressor_candidate.pickle","title":"models"},{"location":"data/companies/","text":"pandas.DataFrame: companies Info <class 'pandas.core.frame.DataFrame'> RangeIndex: 77096 entries, 0 to 77095 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 77096 non-null int64 1 company_rating 47187 non-null object 2 company_location 57966 non-null object 3 total_fleet_count 77089 non-null float64 4 iata_approved 77089 non-null object dtypes: float64(1), int64(1), object(3) memory usage: 2.9+ MB Table Head id company_rating company_location total_fleet_count iata_approved 0 35029 100% Niue 4.00 f 1 30292 67% Anguilla 6.00 f 2 19032 67% Russian Federation 4.00 f 3 8238 91% Barbados 15.00 t 4 30342 NaN Sao Tome and Principe 2.00 t Table Tail id company_rating company_location total_fleet_count iata_approved 77091 6654 100% Tonga 3.00 f 77092 8000 NaN Chile 2.00 t 77093 14296 NaN Netherlands 4.00 f 77094 27363 80% NaN 3.00 t 77095 12542 98% Mauritania 19.00 t Describe id total_fleet_count count 77,096.00 77,089.00 mean 25,155.39 30.47 std 14,300.99 165.55 min 1.00 1.00 25% 12,935.75 1.00 50% 25,253.50 1.00 75% 37,410.25 4.00 max 50,098.00 1,484.00 NaN counts id 0 company_rating 29909 company_location 19130 total_fleet_count 7 iata_approved 7 Unique Values id 50098 company_rating 70 company_location 181 total_fleet_count 90 iata_approved 2","title":"pandas.DataFrame: companies"},{"location":"data/companies/#pandasdataframe-companies","text":"","title":"pandas.DataFrame: companies"},{"location":"data/companies/#info","text":"<class 'pandas.core.frame.DataFrame'> RangeIndex: 77096 entries, 0 to 77095 Data columns (total 5 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 77096 non-null int64 1 company_rating 47187 non-null object 2 company_location 57966 non-null object 3 total_fleet_count 77089 non-null float64 4 iata_approved 77089 non-null object dtypes: float64(1), int64(1), object(3) memory usage: 2.9+ MB","title":"Info"},{"location":"data/companies/#table-head","text":"id company_rating company_location total_fleet_count iata_approved 0 35029 100% Niue 4.00 f 1 30292 67% Anguilla 6.00 f 2 19032 67% Russian Federation 4.00 f 3 8238 91% Barbados 15.00 t 4 30342 NaN Sao Tome and Principe 2.00 t","title":"Table Head"},{"location":"data/companies/#table-tail","text":"id company_rating company_location total_fleet_count iata_approved 77091 6654 100% Tonga 3.00 f 77092 8000 NaN Chile 2.00 t 77093 14296 NaN Netherlands 4.00 f 77094 27363 80% NaN 3.00 t 77095 12542 98% Mauritania 19.00 t","title":"Table Tail"},{"location":"data/companies/#describe","text":"id total_fleet_count count 77,096.00 77,089.00 mean 25,155.39 30.47 std 14,300.99 165.55 min 1.00 1.00 25% 12,935.75 1.00 50% 25,253.50 1.00 75% 37,410.25 4.00 max 50,098.00 1,484.00","title":"Describe"},{"location":"data/companies/#nan-counts","text":"id 0 company_rating 29909 company_location 19130 total_fleet_count 7 iata_approved 7","title":"NaN counts"},{"location":"data/companies/#unique-values","text":"id 50098 company_rating 70 company_location 181 total_fleet_count 90 iata_approved 2","title":"Unique Values"},{"location":"data/model_input_table/","text":"pandas.DataFrame: model_input_table Info <class 'pandas.core.frame.DataFrame'> Int64Index: 759609 entries, 0 to 1864889 Data columns (total 28 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id_x 759609 non-null int64 1 shuttle_location 759609 non-null object 2 shuttle_type 759609 non-null object 3 engine_type 759609 non-null object 4 engine_vendor 759609 non-null object 5 engines 759609 non-null float64 6 passenger_capacity 759609 non-null int64 7 cancellation_policy 759609 non-null object 8 crew 759609 non-null float64 9 d_check_complete 759609 non-null bool 10 moon_clearance_complete 759609 non-null bool 11 price 759609 non-null float64 12 company_id 759609 non-null int64 13 shuttle_id 759609 non-null int64 14 review_scores_rating 759609 non-null float64 15 review_scores_comfort 759609 non-null float64 16 review_scores_amenities 759609 non-null float64 17 review_scores_trip 759609 non-null float64 18 review_scores_crew 759609 non-null float64 19 review_scores_location 759609 non-null float64 20 review_scores_price 759609 non-null float64 21 number_of_reviews 759609 non-null int64 22 reviews_per_month 759609 non-null float64 23 id_y 759609 non-null int64 24 company_rating 759609 non-null float64 25 company_location 759609 non-null object 26 total_fleet_count 759609 non-null float64 27 iata_approved 759609 non-null bool dtypes: bool(3), float64(13), int64(6), object(6) memory usage: 152.9+ MB Table Head id_x shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month id_y company_rating company_location total_fleet_count iata_approved 0 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 1 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 2 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 3 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 4 53260 Niue Type V5 Quantum Banks, Wood and Phillips 1.00 2 strict 1.00 False False 1,325.00 35029 53260 98.00 10.00 9.00 10.00 10.00 9.00 10.00 37 0.48 35029 1.00 Niue 4.00 False Table Tail id_x shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month id_y company_rating company_location total_fleet_count iata_approved 1864307 39094 Russian Federation Type F5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,455.00 42904 39094 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 42904 0.70 Russian Federation 2.00 True 1864357 20330 Uzbekistan Type V5 Quantum ThetaBase Services 1.00 2 flexible 1.00 False False 1,585.00 5701 20330 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 5701 1.00 Costa Rica 1.00 True 1864386 16445 Nicaragua Type V5 Plasma ThetaBase Services 1.00 1 flexible 3.00 False False 1,715.00 13728 16445 100.00 10.00 10.00 10.00 10.00 10.00 10.00 3 3.00 13728 1.00 Pakistan 1.00 False 1864448 76469 Bouvet Island (Bouvetoya) Type V5 Quantum ThetaBase Services 1.00 2 moderate 1.00 False False 1,520.00 41714 76469 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 41714 1.00 Lebanon 1.00 False 1864889 75780 Russian Federation Type V5 Plasma ThetaBase Services 1.00 2 strict 1.00 True False 2,820.00 47766 75780 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 47766 1.00 Uzbekistan 1.00 True Describe id_x engines passenger_capacity crew price company_id shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month id_y company_rating total_fleet_count count 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 mean 38,504.63 2.07 4.72 2.62 3,503.88 26,968.06 38,504.63 88.14 9.05 9.04 9.05 9.13 9.36 8.70 10.30 0.83 26,968.06 0.98 716.92 std 22,169.91 1.28 2.36 1.61 1,866.61 9,058.39 22,169.91 13.42 1.41 1.38 1.52 1.45 1.12 1.42 23.05 1.11 9,058.39 0.07 616.04 min 4.00 0.00 1.00 0.00 870.00 4.00 4.00 20.00 2.00 2.00 2.00 2.00 2.00 2.00 1.00 0.01 4.00 0.00 1.00 25% 19,107.00 1.00 3.00 1.00 2,417.00 22,721.00 19,107.00 80.00 9.00 9.00 9.00 9.00 9.00 8.00 1.00 0.16 22,721.00 1.00 60.00 50% 39,049.00 2.00 4.00 2.00 3,145.00 29,647.00 39,049.00 90.00 10.00 10.00 10.00 10.00 10.00 9.00 3.00 0.40 29,647.00 1.00 1,305.00 75% 58,363.00 3.00 6.00 4.00 4,146.00 29,647.00 58,363.00 100.00 10.00 10.00 10.00 10.00 10.00 10.00 9.00 1.00 29,647.00 1.00 1,305.00 max 77,095.00 12.00 20.00 20.00 86,150.00 50,094.00 77,095.00 100.00 10.00 10.00 10.00 10.00 10.00 10.00 578.00 16.56 50,094.00 1.00 1,484.00 NaN counts id_x 0 shuttle_location 0 shuttle_type 0 engine_type 0 engine_vendor 0 engines 0 passenger_capacity 0 cancellation_policy 0 crew 0 d_check_complete 0 moon_clearance_complete 0 price 0 company_id 0 shuttle_id 0 review_scores_rating 0 review_scores_comfort 0 review_scores_amenities 0 review_scores_trip 0 review_scores_crew 0 review_scores_location 0 review_scores_price 0 number_of_reviews 0 reviews_per_month 0 id_y 0 company_rating 0 company_location 0 total_fleet_count 0 iata_approved 0 Unique Values id_x 29768 shuttle_location 30 shuttle_type 32 engine_type 3 engine_vendor 5 engines 13 passenger_capacity 17 cancellation_policy 3 crew 18 d_check_complete 2 moon_clearance_complete 1 price 527 company_id 15354 shuttle_id 29768 review_scores_rating 54 review_scores_comfort 9 review_scores_amenities 9 review_scores_trip 9 review_scores_crew 9 review_scores_location 8 review_scores_price 9 number_of_reviews 358 reviews_per_month 899 id_y 15354 company_rating 64 company_location 159 total_fleet_count 81 iata_approved 2","title":"pandas.DataFrame: model_input_table"},{"location":"data/model_input_table/#pandasdataframe-model_input_table","text":"","title":"pandas.DataFrame: model_input_table"},{"location":"data/model_input_table/#info","text":"<class 'pandas.core.frame.DataFrame'> Int64Index: 759609 entries, 0 to 1864889 Data columns (total 28 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id_x 759609 non-null int64 1 shuttle_location 759609 non-null object 2 shuttle_type 759609 non-null object 3 engine_type 759609 non-null object 4 engine_vendor 759609 non-null object 5 engines 759609 non-null float64 6 passenger_capacity 759609 non-null int64 7 cancellation_policy 759609 non-null object 8 crew 759609 non-null float64 9 d_check_complete 759609 non-null bool 10 moon_clearance_complete 759609 non-null bool 11 price 759609 non-null float64 12 company_id 759609 non-null int64 13 shuttle_id 759609 non-null int64 14 review_scores_rating 759609 non-null float64 15 review_scores_comfort 759609 non-null float64 16 review_scores_amenities 759609 non-null float64 17 review_scores_trip 759609 non-null float64 18 review_scores_crew 759609 non-null float64 19 review_scores_location 759609 non-null float64 20 review_scores_price 759609 non-null float64 21 number_of_reviews 759609 non-null int64 22 reviews_per_month 759609 non-null float64 23 id_y 759609 non-null int64 24 company_rating 759609 non-null float64 25 company_location 759609 non-null object 26 total_fleet_count 759609 non-null float64 27 iata_approved 759609 non-null bool dtypes: bool(3), float64(13), int64(6), object(6) memory usage: 152.9+ MB","title":"Info"},{"location":"data/model_input_table/#table-head","text":"id_x shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month id_y company_rating company_location total_fleet_count iata_approved 0 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 1 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 2 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 3 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,325.00 35029 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 35029 1.00 Niue 4.00 False 4 53260 Niue Type V5 Quantum Banks, Wood and Phillips 1.00 2 strict 1.00 False False 1,325.00 35029 53260 98.00 10.00 9.00 10.00 10.00 9.00 10.00 37 0.48 35029 1.00 Niue 4.00 False","title":"Table Head"},{"location":"data/model_input_table/#table-tail","text":"id_x shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month id_y company_rating company_location total_fleet_count iata_approved 1864307 39094 Russian Federation Type F5 Quantum ThetaBase Services 1.00 2 strict 1.00 False False 1,455.00 42904 39094 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 42904 0.70 Russian Federation 2.00 True 1864357 20330 Uzbekistan Type V5 Quantum ThetaBase Services 1.00 2 flexible 1.00 False False 1,585.00 5701 20330 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 5701 1.00 Costa Rica 1.00 True 1864386 16445 Nicaragua Type V5 Plasma ThetaBase Services 1.00 1 flexible 3.00 False False 1,715.00 13728 16445 100.00 10.00 10.00 10.00 10.00 10.00 10.00 3 3.00 13728 1.00 Pakistan 1.00 False 1864448 76469 Bouvet Island (Bouvetoya) Type V5 Quantum ThetaBase Services 1.00 2 moderate 1.00 False False 1,520.00 41714 76469 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 41714 1.00 Lebanon 1.00 False 1864889 75780 Russian Federation Type V5 Plasma ThetaBase Services 1.00 2 strict 1.00 True False 2,820.00 47766 75780 100.00 10.00 10.00 10.00 10.00 10.00 10.00 1 1.00 47766 1.00 Uzbekistan 1.00 True","title":"Table Tail"},{"location":"data/model_input_table/#describe","text":"id_x engines passenger_capacity crew price company_id shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month id_y company_rating total_fleet_count count 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 759,609.00 mean 38,504.63 2.07 4.72 2.62 3,503.88 26,968.06 38,504.63 88.14 9.05 9.04 9.05 9.13 9.36 8.70 10.30 0.83 26,968.06 0.98 716.92 std 22,169.91 1.28 2.36 1.61 1,866.61 9,058.39 22,169.91 13.42 1.41 1.38 1.52 1.45 1.12 1.42 23.05 1.11 9,058.39 0.07 616.04 min 4.00 0.00 1.00 0.00 870.00 4.00 4.00 20.00 2.00 2.00 2.00 2.00 2.00 2.00 1.00 0.01 4.00 0.00 1.00 25% 19,107.00 1.00 3.00 1.00 2,417.00 22,721.00 19,107.00 80.00 9.00 9.00 9.00 9.00 9.00 8.00 1.00 0.16 22,721.00 1.00 60.00 50% 39,049.00 2.00 4.00 2.00 3,145.00 29,647.00 39,049.00 90.00 10.00 10.00 10.00 10.00 10.00 9.00 3.00 0.40 29,647.00 1.00 1,305.00 75% 58,363.00 3.00 6.00 4.00 4,146.00 29,647.00 58,363.00 100.00 10.00 10.00 10.00 10.00 10.00 10.00 9.00 1.00 29,647.00 1.00 1,305.00 max 77,095.00 12.00 20.00 20.00 86,150.00 50,094.00 77,095.00 100.00 10.00 10.00 10.00 10.00 10.00 10.00 578.00 16.56 50,094.00 1.00 1,484.00","title":"Describe"},{"location":"data/model_input_table/#nan-counts","text":"id_x 0 shuttle_location 0 shuttle_type 0 engine_type 0 engine_vendor 0 engines 0 passenger_capacity 0 cancellation_policy 0 crew 0 d_check_complete 0 moon_clearance_complete 0 price 0 company_id 0 shuttle_id 0 review_scores_rating 0 review_scores_comfort 0 review_scores_amenities 0 review_scores_trip 0 review_scores_crew 0 review_scores_location 0 review_scores_price 0 number_of_reviews 0 reviews_per_month 0 id_y 0 company_rating 0 company_location 0 total_fleet_count 0 iata_approved 0","title":"NaN counts"},{"location":"data/model_input_table/#unique-values","text":"id_x 29768 shuttle_location 30 shuttle_type 32 engine_type 3 engine_vendor 5 engines 13 passenger_capacity 17 cancellation_policy 3 crew 18 d_check_complete 2 moon_clearance_complete 1 price 527 company_id 15354 shuttle_id 29768 review_scores_rating 54 review_scores_comfort 9 review_scores_amenities 9 review_scores_trip 9 review_scores_crew 9 review_scores_location 8 review_scores_price 9 number_of_reviews 358 reviews_per_month 899 id_y 15354 company_rating 64 company_location 159 total_fleet_count 81 iata_approved 2","title":"Unique Values"},{"location":"data/reviews/","text":"pandas.DataFrame: reviews Info <class 'pandas.core.frame.DataFrame'> RangeIndex: 77096 entries, 0 to 77095 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 shuttle_id 77096 non-null int64 1 review_scores_rating 55956 non-null float64 2 review_scores_comfort 55896 non-null float64 3 review_scores_amenities 55909 non-null float64 4 review_scores_trip 55833 non-null float64 5 review_scores_crew 55902 non-null float64 6 review_scores_location 55831 non-null float64 7 review_scores_price 55828 non-null float64 8 number_of_reviews 77096 non-null int64 9 reviews_per_month 57553 non-null float64 dtypes: float64(8), int64(2) memory usage: 5.9 MB Table Head shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month 0 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 1 36260 90.00 8.00 9.00 10.00 9.00 9.00 9.00 3 0.09 2 57015 95.00 9.00 10.00 9.00 10.00 9.00 9.00 14 0.14 3 14035 93.00 10.00 9.00 9.00 9.00 10.00 9.00 39 0.42 4 10036 98.00 10.00 10.00 10.00 10.00 9.00 9.00 92 0.94 Table Tail shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month 77091 4368 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77092 2983 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77093 69684 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77094 21738 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77095 72645 NaN NaN NaN NaN NaN NaN NaN 0 NaN Describe shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month count 77,096.00 55,956.00 55,896.00 55,909.00 55,833.00 55,902.00 55,831.00 55,828.00 77,096.00 57,553.00 mean 38,548.50 92.76 9.52 9.28 9.64 9.67 9.47 9.28 15.26 1.26 std 22,255.84 9.76 0.96 1.12 0.87 0.85 0.84 1.00 32.20 1.46 min 1.00 20.00 2.00 2.00 2.00 2.00 2.00 2.00 0.00 0.01 25% 19,274.75 90.00 9.00 9.00 10.00 10.00 9.00 9.00 0.00 0.27 50% 38,548.50 96.00 10.00 10.00 10.00 10.00 10.00 10.00 4.00 0.77 75% 57,822.25 100.00 10.00 10.00 10.00 10.00 10.00 10.00 15.00 1.71 max 77,096.00 100.00 10.00 10.00 10.00 10.00 10.00 10.00 578.00 16.56 NaN counts shuttle_id 0 review_scores_rating 21140 review_scores_comfort 21200 review_scores_amenities 21187 review_scores_trip 21263 review_scores_crew 21194 review_scores_location 21265 review_scores_price 21268 number_of_reviews 0 reviews_per_month 19543 Unique Values shuttle_id 77096 review_scores_rating 55 review_scores_comfort 9 review_scores_amenities 9 review_scores_trip 9 review_scores_crew 9 review_scores_location 8 review_scores_price 9 number_of_reviews 369 reviews_per_month 949","title":"pandas.DataFrame: reviews"},{"location":"data/reviews/#pandasdataframe-reviews","text":"","title":"pandas.DataFrame: reviews"},{"location":"data/reviews/#info","text":"<class 'pandas.core.frame.DataFrame'> RangeIndex: 77096 entries, 0 to 77095 Data columns (total 10 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 shuttle_id 77096 non-null int64 1 review_scores_rating 55956 non-null float64 2 review_scores_comfort 55896 non-null float64 3 review_scores_amenities 55909 non-null float64 4 review_scores_trip 55833 non-null float64 5 review_scores_crew 55902 non-null float64 6 review_scores_location 55831 non-null float64 7 review_scores_price 55828 non-null float64 8 number_of_reviews 77096 non-null int64 9 reviews_per_month 57553 non-null float64 dtypes: float64(8), int64(2) memory usage: 5.9 MB","title":"Info"},{"location":"data/reviews/#table-head","text":"shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month 0 63561 97.00 10.00 9.00 10.00 10.00 9.00 10.00 133 1.65 1 36260 90.00 8.00 9.00 10.00 9.00 9.00 9.00 3 0.09 2 57015 95.00 9.00 10.00 9.00 10.00 9.00 9.00 14 0.14 3 14035 93.00 10.00 9.00 9.00 9.00 10.00 9.00 39 0.42 4 10036 98.00 10.00 10.00 10.00 10.00 9.00 9.00 92 0.94","title":"Table Head"},{"location":"data/reviews/#table-tail","text":"shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month 77091 4368 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77092 2983 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77093 69684 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77094 21738 NaN NaN NaN NaN NaN NaN NaN 0 NaN 77095 72645 NaN NaN NaN NaN NaN NaN NaN 0 NaN","title":"Table Tail"},{"location":"data/reviews/#describe","text":"shuttle_id review_scores_rating review_scores_comfort review_scores_amenities review_scores_trip review_scores_crew review_scores_location review_scores_price number_of_reviews reviews_per_month count 77,096.00 55,956.00 55,896.00 55,909.00 55,833.00 55,902.00 55,831.00 55,828.00 77,096.00 57,553.00 mean 38,548.50 92.76 9.52 9.28 9.64 9.67 9.47 9.28 15.26 1.26 std 22,255.84 9.76 0.96 1.12 0.87 0.85 0.84 1.00 32.20 1.46 min 1.00 20.00 2.00 2.00 2.00 2.00 2.00 2.00 0.00 0.01 25% 19,274.75 90.00 9.00 9.00 10.00 10.00 9.00 9.00 0.00 0.27 50% 38,548.50 96.00 10.00 10.00 10.00 10.00 10.00 10.00 4.00 0.77 75% 57,822.25 100.00 10.00 10.00 10.00 10.00 10.00 10.00 15.00 1.71 max 77,096.00 100.00 10.00 10.00 10.00 10.00 10.00 10.00 578.00 16.56","title":"Describe"},{"location":"data/reviews/#nan-counts","text":"shuttle_id 0 review_scores_rating 21140 review_scores_comfort 21200 review_scores_amenities 21187 review_scores_trip 21263 review_scores_crew 21194 review_scores_location 21265 review_scores_price 21268 number_of_reviews 0 reviews_per_month 19543","title":"NaN counts"},{"location":"data/reviews/#unique-values","text":"shuttle_id 77096 review_scores_rating 55 review_scores_comfort 9 review_scores_amenities 9 review_scores_trip 9 review_scores_crew 9 review_scores_location 8 review_scores_price 9 number_of_reviews 369 reviews_per_month 949","title":"Unique Values"},{"location":"data/shuttles/","text":"pandas.DataFrame: shuttles Info <class 'pandas.core.frame.DataFrame'> RangeIndex: 77096 entries, 0 to 77095 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 77096 non-null int64 1 shuttle_location 77096 non-null object 2 shuttle_type 77096 non-null object 3 engine_type 77096 non-null object 4 engine_vendor 77096 non-null object 5 engines 77057 non-null float64 6 passenger_capacity 77096 non-null int64 7 cancellation_policy 77096 non-null object 8 crew 76947 non-null float64 9 d_check_complete 77096 non-null object 10 moon_clearance_complete 77096 non-null object 11 price 77096 non-null object 12 company_id 77096 non-null int64 dtypes: float64(2), int64(3), object(8) memory usage: 7.6+ MB Table Head id shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id 0 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 f f $1,325.0 35029 1 36260 Anguilla Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 t f $1,780.0 30292 2 57015 Russian Federation Type V5 Quantum ThetaBase Services 1.00 2 moderate 0.00 f f $1,715.0 19032 3 14035 Barbados Type V5 Plasma ThetaBase Services 3.00 6 strict 3.00 f f $4,770.0 8238 4 10036 Sao Tome and Principe Type V2 Plasma ThetaBase Services 2.00 4 strict 2.00 f f $2,820.0 30342 Table Tail id shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id 77091 4368 Barbados Type V5 Quantum ThetaBase Services 2.00 4 flexible 2.00 t f $4,107.0 6654 77092 2983 Bouvet Island (Bouvetoya) Type F5 Quantum ThetaBase Services 1.00 1 flexible 1.00 t f $1,169.0 8000 77093 69684 Micronesia Type V5 Plasma ThetaBase Services 0.00 2 flexible 1.00 t f $1,910.0 14296 77094 21738 Uzbekistan Type V5 Plasma ThetaBase Services 1.00 2 flexible 1.00 t f $2,170.0 27363 77095 72645 Malta Type F5 Quantum ThetaBase Services 0.00 2 moderate 2.00 t f $1,455.0 12542 Describe id engines passenger_capacity crew company_id count 77,096.00 77,057.00 77,096.00 76,947.00 77,096.00 mean 38,548.50 1.40 3.16 1.74 25,155.39 std 22,255.84 0.91 1.97 1.24 14,300.99 min 1.00 0.00 1.00 0.00 1.00 25% 19,274.75 1.00 2.00 1.00 12,935.75 50% 38,548.50 1.00 2.00 1.00 25,253.50 75% 57,822.25 2.00 4.00 2.00 37,410.25 max 77,096.00 44.00 20.00 23.00 50,098.00 NaN counts id 0 shuttle_location 0 shuttle_type 0 engine_type 0 engine_vendor 0 engines 39 passenger_capacity 0 cancellation_policy 0 crew 149 d_check_complete 0 moon_clearance_complete 0 price 0 company_id 0 Unique Values id 77096 shuttle_location 30 shuttle_type 42 engine_type 3 engine_vendor 5 engines 15 passenger_capacity 17 cancellation_policy 3 crew 20 d_check_complete 2 moon_clearance_complete 1 price 848 company_id 50098","title":"pandas.DataFrame: shuttles"},{"location":"data/shuttles/#pandasdataframe-shuttles","text":"","title":"pandas.DataFrame: shuttles"},{"location":"data/shuttles/#info","text":"<class 'pandas.core.frame.DataFrame'> RangeIndex: 77096 entries, 0 to 77095 Data columns (total 13 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 id 77096 non-null int64 1 shuttle_location 77096 non-null object 2 shuttle_type 77096 non-null object 3 engine_type 77096 non-null object 4 engine_vendor 77096 non-null object 5 engines 77057 non-null float64 6 passenger_capacity 77096 non-null int64 7 cancellation_policy 77096 non-null object 8 crew 76947 non-null float64 9 d_check_complete 77096 non-null object 10 moon_clearance_complete 77096 non-null object 11 price 77096 non-null object 12 company_id 77096 non-null int64 dtypes: float64(2), int64(3), object(8) memory usage: 7.6+ MB","title":"Info"},{"location":"data/shuttles/#table-head","text":"id shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id 0 63561 Niue Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 f f $1,325.0 35029 1 36260 Anguilla Type V5 Quantum ThetaBase Services 1.00 2 strict 1.00 t f $1,780.0 30292 2 57015 Russian Federation Type V5 Quantum ThetaBase Services 1.00 2 moderate 0.00 f f $1,715.0 19032 3 14035 Barbados Type V5 Plasma ThetaBase Services 3.00 6 strict 3.00 f f $4,770.0 8238 4 10036 Sao Tome and Principe Type V2 Plasma ThetaBase Services 2.00 4 strict 2.00 f f $2,820.0 30342","title":"Table Head"},{"location":"data/shuttles/#table-tail","text":"id shuttle_location shuttle_type engine_type engine_vendor engines passenger_capacity cancellation_policy crew d_check_complete moon_clearance_complete price company_id 77091 4368 Barbados Type V5 Quantum ThetaBase Services 2.00 4 flexible 2.00 t f $4,107.0 6654 77092 2983 Bouvet Island (Bouvetoya) Type F5 Quantum ThetaBase Services 1.00 1 flexible 1.00 t f $1,169.0 8000 77093 69684 Micronesia Type V5 Plasma ThetaBase Services 0.00 2 flexible 1.00 t f $1,910.0 14296 77094 21738 Uzbekistan Type V5 Plasma ThetaBase Services 1.00 2 flexible 1.00 t f $2,170.0 27363 77095 72645 Malta Type F5 Quantum ThetaBase Services 0.00 2 moderate 2.00 t f $1,455.0 12542","title":"Table Tail"},{"location":"data/shuttles/#describe","text":"id engines passenger_capacity crew company_id count 77,096.00 77,057.00 77,096.00 76,947.00 77,096.00 mean 38,548.50 1.40 3.16 1.74 25,155.39 std 22,255.84 0.91 1.97 1.24 14,300.99 min 1.00 0.00 1.00 0.00 1.00 25% 19,274.75 1.00 2.00 1.00 12,935.75 50% 38,548.50 1.00 2.00 1.00 25,253.50 75% 57,822.25 2.00 4.00 2.00 37,410.25 max 77,096.00 44.00 20.00 23.00 50,098.00","title":"Describe"},{"location":"data/shuttles/#nan-counts","text":"id 0 shuttle_location 0 shuttle_type 0 engine_type 0 engine_vendor 0 engines 39 passenger_capacity 0 cancellation_policy 0 crew 149 d_check_complete 0 moon_clearance_complete 0 price 0 company_id 0","title":"NaN counts"},{"location":"data/shuttles/#unique-values","text":"id 77096 shuttle_location 30 shuttle_type 42 engine_type 3 engine_vendor 5 engines 15 passenger_capacity 17 cancellation_policy 3 crew 20 d_check_complete 2 moon_clearance_complete 1 price 848 company_id 50098","title":"Unique Values"},{"location":"parameters/data_science/","text":"data_science Note This page can be autogenerated apart from the general description and those of each parameter. active_modelling_pipeline: Parameters for the currently active model used in production. model_options: Param Value Description test_size 0.2 Percentage of data used for testing. random_state 3 Split Random parameter. features Columns included in model creation. - engines - passenger_capacity - crew - check_complete - moon_clearance_complete - ta_approved - company_rating - review_scores_rating candidate_modelling_pipeline: Parameters used in testing to compare against production model. model_options: Param Value Description test_size 0.2 Percentage of data used for testing. random_state 3 Split Random parameter. features Columns included in model creation. - engines - passenger_capacity - crew - check_complete - review_scores_rating","title":"data_science"},{"location":"parameters/data_science/#data_science","text":"Note This page can be autogenerated apart from the general description and those of each parameter.","title":"data_science"},{"location":"parameters/data_science/#active_modelling_pipeline","text":"Parameters for the currently active model used in production.","title":"active_modelling_pipeline:"},{"location":"parameters/data_science/#model_options","text":"Param Value Description test_size 0.2 Percentage of data used for testing. random_state 3 Split Random parameter. features Columns included in model creation. - engines - passenger_capacity - crew - check_complete - moon_clearance_complete - ta_approved - company_rating - review_scores_rating","title":"model_options:"},{"location":"parameters/data_science/#candidate_modelling_pipeline","text":"Parameters used in testing to compare against production model.","title":"candidate_modelling_pipeline:"},{"location":"parameters/data_science/#model_options_1","text":"Param Value Description test_size 0.2 Percentage of data used for testing. random_state 3 Split Random parameter. features Columns included in model creation. - engines - passenger_capacity - crew - check_complete - review_scores_rating","title":"model_options:"},{"location":"pipelines/_Pipeline_overview/","text":"Description The data_processing pipeline takes in the raw input data Shuttles , Companies and Reviews . Shuttles and Companies require some pre-processing (converting strings to float/boolean values) before the 3 input tables are merged into a single model_input_table to be used in model creation. The data_science pipeline consumes the model_input_table and generates models from it. There are 2 variants, one is the active_modelling_pipeline and the other is a candidate_modelling_pipeline demonstrating the use of modular pipelines and how to use the same code with different sets of parameters. Automatic Doc creation This is a standalone KedroViz React component in a static website. It can be embedded in the document as seen here or accessed as a separate page. KedroViz Pipeline Viewer >>","title":" Pipeline overview"},{"location":"pipelines/_Pipeline_overview/#description","text":"The data_processing pipeline takes in the raw input data Shuttles , Companies and Reviews . Shuttles and Companies require some pre-processing (converting strings to float/boolean values) before the 3 input tables are merged into a single model_input_table to be used in model creation. The data_science pipeline consumes the model_input_table and generates models from it. There are 2 variants, one is the active_modelling_pipeline and the other is a candidate_modelling_pipeline demonstrating the use of modular pipelines and how to use the same code with different sets of parameters. Automatic Doc creation This is a standalone KedroViz React component in a static website. It can be embedded in the document as seen here or accessed as a separate page.","title":"Description"},{"location":"pipelines/_Pipeline_overview/#kedroviz-pipeline-viewer","text":"","title":"KedroViz Pipeline Viewer &gt;&gt;"},{"location":"pipelines/data_processing/","text":"pipeline.data_processing.pipeline.py Automatic Doc creation In this example one still has to create the Inputs and Outputs tables by hand, which is pretty tedius. So we should investigate if this can be a more automated process similar to standard docstrings. create_pipeline ( ** kwargs ) Overview The data_processing pipeline takes in the raw input data and carries out preprocessing to clean up the data nd merge the 3 input tables to a single model_input_table to be used in model creation. Inputs: Name Type Description shuttles pandas.DataFrame List of all shuttles companies pandas.DataFrame List of companies reviews pandas.DataFrame List of reviews Outputs: Name Type Description model_input_table pandas.DataFrame Tidied up and combined list of all shuttles with companies and reviews Source code in src/spaceflights/pipelines/data_processing/pipeline.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def create_pipeline ( ** kwargs ) -> Pipeline : \"\"\" ## Overview The `data_processing` pipeline takes in the raw input data and carries out preprocessing to clean up the data nd merge the 3 input tables to a single `model_input_table` to be used in model creation. ## Inputs: | Name | Type | Description | | --------- | ---------------- | -------------------- | | shuttles | pandas.DataFrame | List of all shuttles | | companies | pandas.DataFrame | List of companies | | reviews | pandas.DataFrame | List of reviews | **Outputs:** | Name | Type | Description | | ----------------- | ---------------- | --------------------------------------- | | model_input_table | pandas.DataFrame | Tidied up and combined list of all </br>shuttles with companies and reviews | \"\"\" return pipeline ( [ node ( func = preprocess_companies , inputs = \"companies\" , outputs = \"preprocessed_companies\" , name = \"preprocess_companies_node\" , ), node ( func = preprocess_shuttles , inputs = \"shuttles\" , outputs = \"preprocessed_shuttles\" , name = \"preprocess_shuttles_node\" , ), node ( func = create_model_input_table , inputs = [ \"preprocessed_shuttles\" , \"preprocessed_companies\" , \"reviews\" ], outputs = \"model_input_table\" , name = \"create_model_input_table_node\" , ), ], namespace = \"data_processing\" , inputs = [ \"companies\" , \"shuttles\" , \"reviews\" ], outputs = \"model_input_table\" , ) pipeline.data_processing.nodes.py Automatic Doc creation Just writing standard docstring is fine for nodes - they are parsed using mkdocstrings and inserted into the main markdown files. create_model_input_table ( shuttles , companies , reviews ) Combines all data to create a model input table. Parameters: Name Type Description Default shuttles pd . DataFrame Preprocessed data for shuttles. required companies pd . DataFrame Preprocessed data for companies. required reviews pd . DataFrame Raw data for reviews. required Returns: Type Description pd . DataFrame Model input table. Source code in src/spaceflights/pipelines/data_processing/nodes.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def create_model_input_table ( shuttles : pd . DataFrame , companies : pd . DataFrame , reviews : pd . DataFrame ) -> pd . DataFrame : \"\"\"Combines all data to create a model input table. Args: shuttles: Preprocessed data for shuttles. companies: Preprocessed data for companies. reviews: Raw data for reviews. Returns: Model input table. \"\"\" rated_shuttles = shuttles . merge ( reviews , left_on = \"id\" , right_on = \"shuttle_id\" ) model_input_table = rated_shuttles . merge ( companies , left_on = \"company_id\" , right_on = \"id\" ) model_input_table = model_input_table . dropna () return model_input_table preprocess_companies ( companies ) Preprocesses the data for companies. Parameters: Name Type Description Default companies pd . DataFrame Raw data. required Returns: Type Description pd . DataFrame Preprocessed data, with company_rating converted to a float and pd . DataFrame iata_approved converted to boolean. Source code in src/spaceflights/pipelines/data_processing/nodes.py 26 27 28 29 30 31 32 33 34 35 36 37 def preprocess_companies ( companies : pd . DataFrame ) -> pd . DataFrame : \"\"\"Preprocesses the data for companies. Args: companies: Raw data. Returns: Preprocessed data, with `company_rating` converted to a float and `iata_approved` converted to boolean. \"\"\" companies [ \"iata_approved\" ] = _is_true ( companies [ \"iata_approved\" ]) companies [ \"company_rating\" ] = _parse_percentage ( companies [ \"company_rating\" ]) return companies preprocess_shuttles ( shuttles ) Preprocesses the data for shuttles. Parameters: Name Type Description Default shuttles pd . DataFrame Raw data. required Returns: Type Description pd . DataFrame Preprocessed data, with price converted to a float and d_check_complete , pd . DataFrame moon_clearance_complete converted to boolean. Source code in src/spaceflights/pipelines/data_processing/nodes.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def preprocess_shuttles ( shuttles : pd . DataFrame ) -> pd . DataFrame : \"\"\"Preprocesses the data for shuttles. Args: shuttles: Raw data. Returns: Preprocessed data, with `price` converted to a float and `d_check_complete`, `moon_clearance_complete` converted to boolean. \"\"\" shuttles [ \"d_check_complete\" ] = _is_true ( shuttles [ \"d_check_complete\" ]) shuttles [ \"moon_clearance_complete\" ] = _is_true ( shuttles [ \"moon_clearance_complete\" ]) shuttles [ \"price\" ] = _parse_money ( shuttles [ \"price\" ]) return shuttles","title":"Data processing"},{"location":"pipelines/data_processing/#pipelinedata_processingpipelinepy","text":"Automatic Doc creation In this example one still has to create the Inputs and Outputs tables by hand, which is pretty tedius. So we should investigate if this can be a more automated process similar to standard docstrings.","title":"pipeline.data_processing.pipeline.py"},{"location":"pipelines/data_processing/#src.spaceflights.pipelines.data_processing.pipeline.create_pipeline","text":"","title":"create_pipeline()"},{"location":"pipelines/data_processing/#src.spaceflights.pipelines.data_processing.pipeline.create_pipeline--overview","text":"The data_processing pipeline takes in the raw input data and carries out preprocessing to clean up the data nd merge the 3 input tables to a single model_input_table to be used in model creation.","title":"Overview"},{"location":"pipelines/data_processing/#src.spaceflights.pipelines.data_processing.pipeline.create_pipeline--inputs","text":"Name Type Description shuttles pandas.DataFrame List of all shuttles companies pandas.DataFrame List of companies reviews pandas.DataFrame List of reviews Outputs: Name Type Description model_input_table pandas.DataFrame Tidied up and combined list of all shuttles with companies and reviews Source code in src/spaceflights/pipelines/data_processing/pipeline.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def create_pipeline ( ** kwargs ) -> Pipeline : \"\"\" ## Overview The `data_processing` pipeline takes in the raw input data and carries out preprocessing to clean up the data nd merge the 3 input tables to a single `model_input_table` to be used in model creation. ## Inputs: | Name | Type | Description | | --------- | ---------------- | -------------------- | | shuttles | pandas.DataFrame | List of all shuttles | | companies | pandas.DataFrame | List of companies | | reviews | pandas.DataFrame | List of reviews | **Outputs:** | Name | Type | Description | | ----------------- | ---------------- | --------------------------------------- | | model_input_table | pandas.DataFrame | Tidied up and combined list of all </br>shuttles with companies and reviews | \"\"\" return pipeline ( [ node ( func = preprocess_companies , inputs = \"companies\" , outputs = \"preprocessed_companies\" , name = \"preprocess_companies_node\" , ), node ( func = preprocess_shuttles , inputs = \"shuttles\" , outputs = \"preprocessed_shuttles\" , name = \"preprocess_shuttles_node\" , ), node ( func = create_model_input_table , inputs = [ \"preprocessed_shuttles\" , \"preprocessed_companies\" , \"reviews\" ], outputs = \"model_input_table\" , name = \"create_model_input_table_node\" , ), ], namespace = \"data_processing\" , inputs = [ \"companies\" , \"shuttles\" , \"reviews\" ], outputs = \"model_input_table\" , )","title":"Inputs:"},{"location":"pipelines/data_processing/#pipelinedata_processingnodespy","text":"Automatic Doc creation Just writing standard docstring is fine for nodes - they are parsed using mkdocstrings and inserted into the main markdown files.","title":"pipeline.data_processing.nodes.py"},{"location":"pipelines/data_processing/#src.spaceflights.pipelines.data_processing.nodes.create_model_input_table","text":"Combines all data to create a model input table. Parameters: Name Type Description Default shuttles pd . DataFrame Preprocessed data for shuttles. required companies pd . DataFrame Preprocessed data for companies. required reviews pd . DataFrame Raw data for reviews. required Returns: Type Description pd . DataFrame Model input table. Source code in src/spaceflights/pipelines/data_processing/nodes.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def create_model_input_table ( shuttles : pd . DataFrame , companies : pd . DataFrame , reviews : pd . DataFrame ) -> pd . DataFrame : \"\"\"Combines all data to create a model input table. Args: shuttles: Preprocessed data for shuttles. companies: Preprocessed data for companies. reviews: Raw data for reviews. Returns: Model input table. \"\"\" rated_shuttles = shuttles . merge ( reviews , left_on = \"id\" , right_on = \"shuttle_id\" ) model_input_table = rated_shuttles . merge ( companies , left_on = \"company_id\" , right_on = \"id\" ) model_input_table = model_input_table . dropna () return model_input_table","title":"create_model_input_table()"},{"location":"pipelines/data_processing/#src.spaceflights.pipelines.data_processing.nodes.preprocess_companies","text":"Preprocesses the data for companies. Parameters: Name Type Description Default companies pd . DataFrame Raw data. required Returns: Type Description pd . DataFrame Preprocessed data, with company_rating converted to a float and pd . DataFrame iata_approved converted to boolean. Source code in src/spaceflights/pipelines/data_processing/nodes.py 26 27 28 29 30 31 32 33 34 35 36 37 def preprocess_companies ( companies : pd . DataFrame ) -> pd . DataFrame : \"\"\"Preprocesses the data for companies. Args: companies: Raw data. Returns: Preprocessed data, with `company_rating` converted to a float and `iata_approved` converted to boolean. \"\"\" companies [ \"iata_approved\" ] = _is_true ( companies [ \"iata_approved\" ]) companies [ \"company_rating\" ] = _parse_percentage ( companies [ \"company_rating\" ]) return companies","title":"preprocess_companies()"},{"location":"pipelines/data_processing/#src.spaceflights.pipelines.data_processing.nodes.preprocess_shuttles","text":"Preprocesses the data for shuttles. Parameters: Name Type Description Default shuttles pd . DataFrame Raw data. required Returns: Type Description pd . DataFrame Preprocessed data, with price converted to a float and d_check_complete , pd . DataFrame moon_clearance_complete converted to boolean. Source code in src/spaceflights/pipelines/data_processing/nodes.py 40 41 42 43 44 45 46 47 48 49 50 51 52 def preprocess_shuttles ( shuttles : pd . DataFrame ) -> pd . DataFrame : \"\"\"Preprocesses the data for shuttles. Args: shuttles: Raw data. Returns: Preprocessed data, with `price` converted to a float and `d_check_complete`, `moon_clearance_complete` converted to boolean. \"\"\" shuttles [ \"d_check_complete\" ] = _is_true ( shuttles [ \"d_check_complete\" ]) shuttles [ \"moon_clearance_complete\" ] = _is_true ( shuttles [ \"moon_clearance_complete\" ]) shuttles [ \"price\" ] = _parse_money ( shuttles [ \"price\" ]) return shuttles","title":"preprocess_shuttles()"},{"location":"pipelines/data_science/","text":"pipeline.data_science.pipeline.py Automatic Doc creation In this example one still has to create the Inputs and Outputs tables by hand, which is pretty tedius. So to investigate if this can be a more automated process similar to standard docstrings. create_pipeline ( ** kwargs ) Overview The data_science pipeline uses the model_input_table and splits the dataset into a train and test set and then uses LinearRegression to build a model to predict flight prices. It then evaluates the model and prints the result to the log. It creates 2 instances of the pipelines with independent parameters, active_modelling_pipeline and candidate_modelling_pipeline . Inputs: Name Type Description model_input_table pandas.DataFrame Tidied up and combined list of all shuttles with companies and reviews Outputs: Name Type Description active_modelling_pipeline.regressor pickle.PickleDataSet Active model in production candidate_modelling_pipeline.regressor pickle.PickleDataSet Candidate model in development Source code in src/spaceflights/pipelines/data_science/pipeline.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def create_pipeline ( ** kwargs ) -> Pipeline : \"\"\" ## Overview The `data_science` pipeline uses the `model_input_table` and splits the dataset into a train and test set and then uses `LinearRegression` to build a model to predict flight prices. It then evaluates the model and prints the result to the log. It creates 2 instances of the pipelines with independent parameters, `active_modelling_pipeline` and `candidate_modelling_pipeline`. ## Inputs: | Name | Type | Description | | ------------------- | ------------------ | --------------------------------------- | | `model_input_table` | `pandas.DataFrame` | Tidied up and combined list of all </br>shuttles with companies and reviews | **Outputs:** | Name | Type | Description | | -------------------------------------- | -------------------- | --------------------------------------- | | `active_modelling_pipeline.regressor` | `pickle.PickleDataSet` | Active model in production | | `candidate_modelling_pipeline.regressor` | `pickle.PickleDataSet` | Candidate model in development | \"\"\" pipeline_instance = pipeline ( [ node ( func = split_data , inputs = [ \"model_input_table\" , \"params:model_options\" ], outputs = [ \"X_train\" , \"X_test\" , \"y_train\" , \"y_test\" ], name = \"split_data_node\" , ), node ( func = train_model , inputs = [ \"X_train\" , \"y_train\" ], outputs = \"regressor\" , name = \"train_model_node\" , ), node ( func = evaluate_model , inputs = [ \"regressor\" , \"X_test\" , \"y_test\" ], outputs = None , name = \"evaluate_model_node\" , ), ] ) ds_pipeline_1 = pipeline ( pipe = pipeline_instance , inputs = \"model_input_table\" , namespace = \"active_modelling_pipeline\" , ) ds_pipeline_2 = pipeline ( pipe = pipeline_instance , inputs = \"model_input_table\" , namespace = \"candidate_modelling_pipeline\" , ) return pipeline ( pipe = ds_pipeline_1 + ds_pipeline_2 , inputs = \"model_input_table\" , namespace = \"data_science\" , ) pipeline.data_science.nodes.py Automatic Doc creation Just writing standard docstring is fine for nodes - they are parsed using mkdocstrings and inserted into the main markdown files. evaluate_model ( regressor , X_test , y_test ) Calculates and logs the coefficient of determination. Parameters: Name Type Description Default regressor LinearRegression Trained model. required X_test pd . DataFrame Testing data of independent features. required y_test pd . Series Testing data for price. required Source code in src/spaceflights/pipelines/data_science/nodes.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def evaluate_model ( regressor : LinearRegression , X_test : pd . DataFrame , y_test : pd . Series ): \"\"\"Calculates and logs the coefficient of determination. Args: regressor: Trained model. X_test: Testing data of independent features. y_test: Testing data for price. \"\"\" y_pred = regressor . predict ( X_test ) score = r2_score ( y_test , y_pred ) logger = logging . getLogger ( __name__ ) logger . info ( \"Model has a coefficient R^2 of %.3f on test data.\" , score ) split_data ( data , parameters ) Splits data into features and targets training and test sets. Parameters: Name Type Description Default data pd . DataFrame Data containing features and target. required parameters Dict Parameters defined in parameters/data_science.yml. required Returns: Type Description Tuple Split data. Source code in src/spaceflights/pipelines/data_science/nodes.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def split_data ( data : pd . DataFrame , parameters : Dict ) -> Tuple : \"\"\"Splits data into features and targets training and test sets. Args: data: Data containing features and target. parameters: Parameters defined in parameters/data_science.yml. Returns: Split data. \"\"\" X = data [ parameters [ \"features\" ]] y = data [ \"price\" ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = parameters [ \"test_size\" ], random_state = parameters [ \"random_state\" ] ) return X_train , X_test , y_train , y_test train_model ( X_train , y_train ) Trains the linear regression model. Parameters: Name Type Description Default X_train pd . DataFrame Training data of independent features. required y_train pd . Series Training data for price. required Returns: Type Description LinearRegression Trained model. Source code in src/spaceflights/pipelines/data_science/nodes.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def train_model ( X_train : pd . DataFrame , y_train : pd . Series ) -> LinearRegression : \"\"\"Trains the linear regression model. Args: X_train: Training data of independent features. y_train: Training data for price. Returns: Trained model. \"\"\" regressor = LinearRegression () regressor . fit ( X_train , y_train ) return regressor","title":"Data science"},{"location":"pipelines/data_science/#pipelinedata_sciencepipelinepy","text":"Automatic Doc creation In this example one still has to create the Inputs and Outputs tables by hand, which is pretty tedius. So to investigate if this can be a more automated process similar to standard docstrings.","title":"pipeline.data_science.pipeline.py"},{"location":"pipelines/data_science/#src.spaceflights.pipelines.data_science.pipeline.create_pipeline","text":"","title":"create_pipeline()"},{"location":"pipelines/data_science/#src.spaceflights.pipelines.data_science.pipeline.create_pipeline--overview","text":"The data_science pipeline uses the model_input_table and splits the dataset into a train and test set and then uses LinearRegression to build a model to predict flight prices. It then evaluates the model and prints the result to the log. It creates 2 instances of the pipelines with independent parameters, active_modelling_pipeline and candidate_modelling_pipeline .","title":"Overview"},{"location":"pipelines/data_science/#src.spaceflights.pipelines.data_science.pipeline.create_pipeline--inputs","text":"Name Type Description model_input_table pandas.DataFrame Tidied up and combined list of all shuttles with companies and reviews Outputs: Name Type Description active_modelling_pipeline.regressor pickle.PickleDataSet Active model in production candidate_modelling_pipeline.regressor pickle.PickleDataSet Candidate model in development Source code in src/spaceflights/pipelines/data_science/pipeline.py 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def create_pipeline ( ** kwargs ) -> Pipeline : \"\"\" ## Overview The `data_science` pipeline uses the `model_input_table` and splits the dataset into a train and test set and then uses `LinearRegression` to build a model to predict flight prices. It then evaluates the model and prints the result to the log. It creates 2 instances of the pipelines with independent parameters, `active_modelling_pipeline` and `candidate_modelling_pipeline`. ## Inputs: | Name | Type | Description | | ------------------- | ------------------ | --------------------------------------- | | `model_input_table` | `pandas.DataFrame` | Tidied up and combined list of all </br>shuttles with companies and reviews | **Outputs:** | Name | Type | Description | | -------------------------------------- | -------------------- | --------------------------------------- | | `active_modelling_pipeline.regressor` | `pickle.PickleDataSet` | Active model in production | | `candidate_modelling_pipeline.regressor` | `pickle.PickleDataSet` | Candidate model in development | \"\"\" pipeline_instance = pipeline ( [ node ( func = split_data , inputs = [ \"model_input_table\" , \"params:model_options\" ], outputs = [ \"X_train\" , \"X_test\" , \"y_train\" , \"y_test\" ], name = \"split_data_node\" , ), node ( func = train_model , inputs = [ \"X_train\" , \"y_train\" ], outputs = \"regressor\" , name = \"train_model_node\" , ), node ( func = evaluate_model , inputs = [ \"regressor\" , \"X_test\" , \"y_test\" ], outputs = None , name = \"evaluate_model_node\" , ), ] ) ds_pipeline_1 = pipeline ( pipe = pipeline_instance , inputs = \"model_input_table\" , namespace = \"active_modelling_pipeline\" , ) ds_pipeline_2 = pipeline ( pipe = pipeline_instance , inputs = \"model_input_table\" , namespace = \"candidate_modelling_pipeline\" , ) return pipeline ( pipe = ds_pipeline_1 + ds_pipeline_2 , inputs = \"model_input_table\" , namespace = \"data_science\" , )","title":"Inputs:"},{"location":"pipelines/data_science/#pipelinedata_sciencenodespy","text":"Automatic Doc creation Just writing standard docstring is fine for nodes - they are parsed using mkdocstrings and inserted into the main markdown files.","title":"pipeline.data_science.nodes.py"},{"location":"pipelines/data_science/#src.spaceflights.pipelines.data_science.nodes.evaluate_model","text":"Calculates and logs the coefficient of determination. Parameters: Name Type Description Default regressor LinearRegression Trained model. required X_test pd . DataFrame Testing data of independent features. required y_test pd . Series Testing data for price. required Source code in src/spaceflights/pipelines/data_science/nodes.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 def evaluate_model ( regressor : LinearRegression , X_test : pd . DataFrame , y_test : pd . Series ): \"\"\"Calculates and logs the coefficient of determination. Args: regressor: Trained model. X_test: Testing data of independent features. y_test: Testing data for price. \"\"\" y_pred = regressor . predict ( X_test ) score = r2_score ( y_test , y_pred ) logger = logging . getLogger ( __name__ ) logger . info ( \"Model has a coefficient R^2 of %.3f on test data.\" , score )","title":"evaluate_model()"},{"location":"pipelines/data_science/#src.spaceflights.pipelines.data_science.nodes.split_data","text":"Splits data into features and targets training and test sets. Parameters: Name Type Description Default data pd . DataFrame Data containing features and target. required parameters Dict Parameters defined in parameters/data_science.yml. required Returns: Type Description Tuple Split data. Source code in src/spaceflights/pipelines/data_science/nodes.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 def split_data ( data : pd . DataFrame , parameters : Dict ) -> Tuple : \"\"\"Splits data into features and targets training and test sets. Args: data: Data containing features and target. parameters: Parameters defined in parameters/data_science.yml. Returns: Split data. \"\"\" X = data [ parameters [ \"features\" ]] y = data [ \"price\" ] X_train , X_test , y_train , y_test = train_test_split ( X , y , test_size = parameters [ \"test_size\" ], random_state = parameters [ \"random_state\" ] ) return X_train , X_test , y_train , y_test","title":"split_data()"},{"location":"pipelines/data_science/#src.spaceflights.pipelines.data_science.nodes.train_model","text":"Trains the linear regression model. Parameters: Name Type Description Default X_train pd . DataFrame Training data of independent features. required y_train pd . Series Training data for price. required Returns: Type Description LinearRegression Trained model. Source code in src/spaceflights/pipelines/data_science/nodes.py 34 35 36 37 38 39 40 41 42 43 44 45 46 def train_model ( X_train : pd . DataFrame , y_train : pd . Series ) -> LinearRegression : \"\"\"Trains the linear regression model. Args: X_train: Training data of independent features. y_train: Training data for price. Returns: Trained model. \"\"\" regressor = LinearRegression () regressor . fit ( X_train , y_train ) return regressor","title":"train_model()"}]}